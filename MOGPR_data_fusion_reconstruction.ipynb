{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnVZEdaf4PiadUutFJmFwW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daviddkovacs/MOGPR-time-series-reconstruction/blob/main/MOGPR_data_fusion_reconstruction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installing required libraries"
      ],
      "metadata": {
        "id": "Kbugpm2PaCKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "\n",
        "!pip install xesmf\n",
        "!pip install wxee\n",
        "!pip install rasterio\n",
        "!pip install geopandas\n",
        "!pip install geextract\n",
        "!pip install gmaps\n",
        "!pip install pdf2image\n",
        "!pip install mogptk\n",
        "!pip install GPy\n",
        "!pip install openeo\n",
        "!pip install PyPDF2\n",
        "\n",
        "ee.Initialize()"
      ],
      "metadata": {
        "id": "xrakbutLaAO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cwpuaW_XZx2L"
      },
      "outputs": [],
      "source": [
        "from geextract import ts_extract, relabel, date_append, dictlist2sqlite\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import osgeo.ogr\n",
        "import json\n",
        "from pdf2image import convert_from_path\n",
        "import sys\n",
        "import pickle\n",
        "import GPy\n",
        "import openeo\n",
        "import h5py\n",
        "import math\n",
        "from scipy.stats import pearsonr\n",
        "import csv\n",
        "import PyPDF2\n",
        "from shapely.geometry import box\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from openeo.rest.conversions import timeseries_json_to_pandas\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import time\n",
        "import os\n",
        "import gmaps\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as ticker\n",
        "import traceback\n",
        "from datetime import timedelta\n",
        "import shapely\n",
        "from shapely.geometry import mapping\n",
        "from scipy.stats.stats import pearsonr, spearmanr\n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define temporal range\n",
        "\n",
        "\n",
        "if you define, in the first cells, the temporal domain to be the same, i.e. 2019 for S3 and 2019 for MODIS, it will be gap filling\n",
        "\n",
        "if you define MODIS to be, let's say, 2010-2020 and S3 to be only 2019, then it will reconstruct/extend S3 to the broader, 2010-20 timeframe\n",
        "i.e temporally reconstruct."
      ],
      "metadata": {
        "id": "YQcZhTDhaYr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = datetime(2000, 1, 1)\n",
        "end_date   = datetime(2022, 1, 1)\n",
        "\n",
        "# Available dates for S3 based vegetation traits: 2017-01-01 to 2021-12-01\n",
        "start_date_S3 = datetime(2019, 1, 1)\n",
        "end_date_S3   = datetime(2020, 1, 1)\n",
        "\n",
        "Tstep      = 8"
      ],
      "metadata": {
        "id": "-csq2fvGaNNA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define spatial domain\n",
        "\n",
        "Adjust site as needed. This site is defaulted to Valencia Anchor Station"
      ],
      "metadata": {
        "id": "fRRq01-saaFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "site ={'geometry':{'coordinates':\n",
        "         [[[-1.296402507606762, 39.57334640048957],\n",
        "          [-1.296402507606762, 39.56600234855508],\n",
        "          [-1.2870469625628167, 39.56600234855508],\n",
        "          [-1.2870469625628167, 39.57334640048957]]]\n",
        "                     },u'type': u'Polygon'}\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "bMPJrOV1aYMJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create the 8 day temporal composites of MODIS NDVI. This dataset will be used as a guiding, predictor variable later on."
      ],
      "metadata": {
        "id": "TzHyIVwIar1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_collection = (\n",
        "    ee.ImageCollection(\"MODIS/MOD09GA_006_NDVI\")\n",
        "    .filterDate(start_date, end_date)\n",
        ")\n",
        "\n",
        "# Create a list of dates at 8-day intervals\n",
        "dates = ee.List.sequence(\n",
        "    ee.Date(start_date).millis(),\n",
        "    ee.Date(end_date).advance(1, 'day').millis(),  # Add 1 day to include the end_date\n",
        "    8 * 24 * 60 * 60 * 1000  # 8 days in milliseconds\n",
        ")\n",
        "\n",
        "# function to create an 8-day mean composite\n",
        "def create_8_day_composite(start_date):\n",
        "    start_date = ee.Date(start_date)\n",
        "    end_date = start_date.advance(8, 'day')\n",
        "\n",
        "    mean_image = (\n",
        "        daily_collection\n",
        "        .filterDate(start_date, end_date)\n",
        "        .mean()\n",
        "    )\n",
        "    system_id = start_date.format('YYYY_MM_dd')\n",
        "\n",
        "    mean_image = mean_image.set('system:time_start', start_date)\n",
        "\n",
        "    return mean_image.set('system:id', system_id)\n",
        "\n",
        "composite_collection = ee.ImageCollection(dates.map(create_8_day_composite))"
      ],
      "metadata": {
        "id": "qK3ZFtfVaktq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_empty_2D_list(nrows,ncols):\n",
        "\n",
        "    empty_list = []\n",
        "    for j in range(nrows):\n",
        "        column = []\n",
        "        for i in range(ncols):\n",
        "            column.append(0)\n",
        "        empty_list.append(column)\n",
        "    return empty_list\n",
        "\n",
        "def S3TOA_get_date(id):\n",
        "\n",
        "    date_info = datetime.date(datetime(int(id[0:4]),\n",
        "                                       int(id[4:6]),\n",
        "                                       int(id[6:8])))\n",
        "    return date_info\n",
        "\n",
        "\n",
        "def MODIS_get_date(id):\n",
        "\n",
        "    date_info = datetime.date(datetime(int(id[0:4]), int(id[5:7]), int(id[8:])))\n",
        "\n",
        "    return date_info\n",
        "\n",
        "def MODISNDVI_get_date(id):\n",
        "\n",
        "    day_from_start = start_date + timedelta(days=int(id)*8)\n",
        "    date_info = day_from_start\n",
        "\n",
        "    return date_info\n",
        "\n",
        "\n",
        "def extract_S3TOA_time_series(S3_LCC_flag,\n",
        "                              S3_LAI_flag,\n",
        "                              S3_FAPAR_flag,\n",
        "                              S3_FVC_flag,\n",
        "                              feature,start_date = [],end_date = [],stats='mean',pix_res=500\n",
        "                              ):\n",
        "\n",
        "    global product_name\n",
        "    global exp_var_unit\n",
        "    global exp_variable\n",
        "    global plotylim_min\n",
        "    global plotylim_max\n",
        "\n",
        "    if S3_LCC_flag:\n",
        "\n",
        "        product_name = 'projects/ee-dkvcsdvd/assets/EVT_regional/LCC_10_sites'\n",
        "        band_name = ['LCC_GREEN']\n",
        "        bands_name_out = ['LAI']\n",
        "        exp_variable = \"LCC\"\n",
        "        exp_var_unit = \"$\\mu$g/cm$^2$\"\n",
        "        plotylim_min = -7\n",
        "        plotylim_max = 100\n",
        "\n",
        "    if S3_LAI_flag:\n",
        "\n",
        "        product_name = 'projects/ee-dkvcsdvd/assets/Paper3_GBOV/S3_LAI'\n",
        "        band_name = ['LAI_GREEN']\n",
        "        bands_name_out = ['LAI']\n",
        "        exp_variable = \"LAI\"\n",
        "        exp_var_unit = \"m$^2$/m$^2$\"\n",
        "        plotylim_min = -0.5\n",
        "        plotylim_max = 9\n",
        "\n",
        "    if S3_FAPAR_flag:\n",
        "\n",
        "        product_name = 'projects/ee-dkvcsdvd/assets/Paper3_GBOV/S3_FAPAR'\n",
        "        band_name = ['FAPAR_GREEN']\n",
        "        bands_name_out = ['LAI']\n",
        "        exp_variable = \"FAPAR\"\n",
        "        exp_var_unit = \"[-]\"\n",
        "        plotylim_min = 0\n",
        "        plotylim_max = 1\n",
        "\n",
        "    if S3_FVC_flag:\n",
        "\n",
        "        product_name = 'projects/ee-dkvcsdvd/assets/Paper3_GBOV/S3_FVC'\n",
        "        band_name = ['FVC_GREEN']\n",
        "        bands_name_out = ['LAI']\n",
        "        exp_variable = \"FVC\"\n",
        "        exp_var_unit = \"[-]\"\n",
        "        plotylim_min = 0\n",
        "        plotylim_max = 1\n",
        "\n",
        "    geometry = ee.Geometry.Polygon(feature['geometry']['coordinates'])\n",
        "    coll = ee.ImageCollection(product_name)\\\n",
        "        .filterBounds(geometry)\\\n",
        "        .filterDate(start_date_S3, end_date_S3)\\\n",
        "        .select(band_name,bands_name_out)\n",
        "\n",
        "    images_list = [item.get('id') for item in coll.getInfo().get('features')]\n",
        "\n",
        "    info = coll.getRegion(geometry,pix_res).getInfo()\n",
        "\n",
        "    df=pd.DataFrame(info[1:],columns=info[0]).groupby('id').mean().reset_index()\n",
        "\n",
        "    df['date']=df['id'].apply(lambda x: S3TOA_get_date(x))\n",
        "\n",
        "    return df,images_list\n",
        "\n",
        "def extract_MODISLAI_time_series(feature,start_date = [],end_date = [],stats='mean',pix_res=500):\n",
        "\n",
        "    product_name = 'MODIS/061/MCD15A3H'\n",
        "    band_name = ['Lai']\n",
        "    bands_name_out = ['LAI']\n",
        "\n",
        "    geometry = ee.Geometry.Polygon(feature['geometry']['coordinates'])\n",
        "\n",
        "    coll = ee.ImageCollection(product_name)\\\n",
        "        .filterBounds(geometry)\\\n",
        "        .filterDate(start_date, end_date)\\\n",
        "        .select(band_name,bands_name_out)\n",
        "\n",
        "    images_list = [item.get('id') for item in coll.getInfo().get('features')]\n",
        "\n",
        "    info = coll.getRegion(geometry,pix_res).getInfo()\n",
        "\n",
        "    df=pd.DataFrame(info[1:],columns=info[0]).groupby('id').mean().reset_index() # Used to be .mean()\n",
        "\n",
        "    df['date']=df['id'].apply(lambda x: MODIS_get_date(x))\n",
        "\n",
        "    return df,images_list\n",
        "\n",
        "def extract_MODISFAPAR_time_series(feature,start_date = [],end_date = [],stats='mean',pix_res=500):\n",
        "\n",
        "    product_name = 'MODIS/061/MCD15A3H'\n",
        "    band_name = ['Fpar']\n",
        "    bands_name_out = ['FAPAR']\n",
        "\n",
        "    geometry = ee.Geometry.Polygon(feature['geometry']['coordinates'])\n",
        "\n",
        "    coll = ee.ImageCollection(product_name)\\\n",
        "        .filterBounds(geometry)\\\n",
        "        .filterDate(start_date, end_date)\\\n",
        "        .select(band_name,bands_name_out)\n",
        "\n",
        "    images_list = [item.get('id') for item in coll.getInfo().get('features')]\n",
        "\n",
        "    info = coll.getRegion(geometry,pix_res).getInfo()\n",
        "\n",
        "    df=pd.DataFrame(info[1:],columns=info[0]).groupby('id').mean().reset_index()\n",
        "\n",
        "    df['date']=df['id'].apply(lambda x: MODIS_get_date(x))\n",
        "\n",
        "    return df,images_list\n",
        "\n",
        "\n",
        "def extract_MODISNDVI_time_series(feature,start_date = [],end_date = [],stats='mean',pix_res=500):\n",
        "\n",
        "    product_name = composite_collection\n",
        "    band_name = ['NDVI']\n",
        "    bands_name_out = ['NDVI']\n",
        "\n",
        "    geometry = ee.Geometry.Polygon(feature['geometry']['coordinates'])\n",
        "\n",
        "    coll = ee.ImageCollection(product_name)\\\n",
        "        .filterBounds(geometry)\\\n",
        "        .filterDate(start_date, end_date)\\\n",
        "        .select(band_name,bands_name_out)\n",
        "\n",
        "    images_list = [item.get('id') for item in coll.getInfo().get('features')]\n",
        "\n",
        "    info = coll.getRegion(geometry,pix_res).getInfo()\n",
        "\n",
        "    df=pd.DataFrame(info[1:],columns=info[0]).groupby('id').mean().reset_index() # Used to be mean but it's daily data an error when accumulating elements\n",
        "\n",
        "    df['date']=df['id'].apply(lambda x: MODISNDVI_get_date(x))\n",
        "\n",
        "    return df,images_list\n",
        "\n",
        "def need_to_name(df_in):\n",
        "    df_in['LAI'] = df_in.LAI\n",
        "    return df_in\n",
        "\n",
        "def need_to_name_CGLS_LAI(df_in):\n",
        "    df_in['b1'] = df_in.b1 /31.875\n",
        "    return df_in\n",
        "\n",
        "def need_to_name_CGLS(df_in):\n",
        "    df_in['b1'] = df_in.b1 /255\n",
        "    return df_in\n",
        "\n",
        "def need_to_name_merisdivLCC(df_in):\n",
        "    df_in['b1'] = (df_in.b1) / 100\n",
        "    return df_in\n",
        "\n",
        "def need_to_name_gbov(df_in):\n",
        "    df_in['b1'] = df_in.b1\n",
        "    return df_in\n",
        "\n",
        "def need_to_name_modisdivLCC(df_in):\n",
        "    df_in['b1'] = (df_in.b1) / 2\n",
        "    return df_in\n",
        "\n",
        "def need_to_name_modisdivLAI(df_in):\n",
        "    df_in['LAI'] = (df_in.LAI) / 10\n",
        "    return df_in\n",
        "\n",
        "def need_to_name_modisdivFAPAR(df_in):\n",
        "    df_in['FAPAR'] = (df_in.FAPAR) / 100\n",
        "    return df_in\n",
        "\n",
        "def need_to_name_modisdivNDVI(df_in):\n",
        "    df_in['NDVI'] = (df_in.NDVI)\n",
        "    return df_in\n",
        "\n",
        "\n",
        "def datestr_to_number(time_vec):\n",
        "\n",
        "    \"\"\"\n",
        "    Function coverting date vector from string to absolute number vector\n",
        "    \"\"\"\n",
        "\n",
        "    time_vec_num = np.asarray([ _.toordinal() for _ in time_vec], dtype=np.float32)\n",
        "    return time_vec_num\n",
        "\n",
        "def number_to_datestr(time_vec_num):\n",
        "\n",
        "    \"\"\"\n",
        "    Function coverting date vector from string to absolute number vector\n",
        "    \"\"\"\n",
        "\n",
        "    time_vec = [datetime.fromordinal(int(_)) for _ in time_vec_num]\n",
        "    return time_vec\n",
        "\n",
        "def GPY_retrieval_Noutput(DATA,TIME,Master_Ind,output_timevec,Nt,proc_line_print ='MOGPR modelling..',W_rank=1, trained_model=[]):\n",
        "    \"\"\"\n",
        "    Function performing the multioutput gaussian-process regression at pixel level for gapfilling and temporal reconstruction purposes\n",
        "\n",
        "    Args:\n",
        "        DATA [array] : 3D (2DSpace, Time) array containing data to be processed\n",
        "        TIME [array] : vector containing the dates of each layer in the time dimension\n",
        "        Master_Ind [int] : Index identifying the Master output\n",
        "        output_timevec [array] :vector containing the dates on which output must be estimated\n",
        "        Nt [int]  : # of time the GP training must be performed (def=1)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    noutput_timeseries = len(DATA)\n",
        "\n",
        "    x_size = DATA[0].shape[1]\n",
        "    y_size = DATA[0].shape[2]\n",
        "    imout_sz = (output_timevec.shape[0],x_size,y_size)\n",
        "\n",
        "    Xtest     = output_timevec.reshape(output_timevec.shape[0],1)\n",
        "    Out_QFlag = np.ones((x_size,y_size), dtype=bool)\n",
        "    Out_mean  = []\n",
        "    Out_unc   = []\n",
        "    Out_model = create_empty_2D_list(x_size,y_size)\n",
        "\n",
        "    for _ in range(noutput_timeseries):\n",
        "        Out_mean.append(np.full(imout_sz,np.nan))\n",
        "        Out_unc.append(np.full(imout_sz,np.nan))\n",
        "\n",
        "    model_parameter_names = None\n",
        "    cnt = 0\n",
        "    tot = x_size*y_size\n",
        "    check_param_names_flag= True\n",
        "\n",
        "    for x, y in itertools.product(range(x_size), range(y_size)):\n",
        "\n",
        "            X_vec = []\n",
        "            Y_vec = []\n",
        "            Y_mean_vec = []\n",
        "            Y_std_vec  = []\n",
        "\n",
        "            for ind in range(noutput_timeseries):\n",
        "\n",
        "              #X_tmp  = np.array(TIME[ind],dtype=np.float128)\n",
        "              #Y_tmp  =np.array( DATA[ind][:,x,y],dtype=np.float128)\n",
        "              X_tmp  = TIME[ind]\n",
        "              Y_tmp  = DATA[ind][:,x,y]\n",
        "              X_tmp  = X_tmp[~np.isnan(Y_tmp),np.newaxis]\n",
        "              Y_tmp  = Y_tmp[~np.isnan(Y_tmp),np.newaxis]\n",
        "              X_vec.append(X_tmp)\n",
        "              Y_vec.append(Y_tmp)\n",
        "              del X_tmp,Y_tmp\n",
        "\n",
        "\n",
        "            if np.size(Y_vec[Master_Ind]) >0:\n",
        "\n",
        "                # Data Normalization\n",
        "                for ind in range(noutput_timeseries):\n",
        "                    Y_mean_vec.append(np.mean(Y_vec[ind]))\n",
        "                    Y_std_vec.append(np.std(Y_vec[ind]))\n",
        "                    Y_vec[ind] = (Y_vec[ind]-Y_mean_vec[ind])/Y_std_vec[ind]\n",
        "\n",
        "                # Multi-output training and testing sets\n",
        "                Xtrain = X_vec\n",
        "                Ytrain = Y_vec\n",
        "\n",
        "                nsamples, npixels = Xtest.shape\n",
        "                noutputs = len(Ytrain)\n",
        "\n",
        "                for i_test in range(Nt):\n",
        "\n",
        "                    Yp = np.zeros((nsamples, noutputs))\n",
        "                    Vp = np.zeros((nsamples, noutputs))\n",
        "\n",
        "                    K            =  GPy.kern.Matern32(Xtrain[0].shape[1])  # Use RBF or Matern32 as kernels\n",
        "                    LCM          =  GPy.util.multioutput.LCM(input_dim    =  Xtrain[0].shape[1],\n",
        "                                                             num_outputs  =  noutputs,\n",
        "                                                             kernels_list =  [K] * noutputs, W_rank=W_rank)\n",
        "\n",
        "                    model = GPy.models.GPCoregionalizedRegression(Xtrain, Ytrain, kernel=LCM.copy())  # , W_rank=noutputs)\n",
        "                    model['.*Mat32.var'].constrain_fixed(1.)\n",
        "\n",
        "                    if not np.isnan(Ytrain[1]).all():\n",
        "\n",
        "                            try:\n",
        "                                #if trained_model is None:\n",
        "                                model.optimize()\n",
        "                                #else:\n",
        "                                    #here we should iterate through the model's parameters to assigned the pretrained values\n",
        "                                   # model = 1\n",
        "\n",
        "                                list_tmp = [model.param_array]\n",
        "\n",
        "                                for _ in range(noutput_timeseries):\n",
        "                                    #print('model.sum.ICM'+str(_)+'.B.B')\n",
        "                                    list_tmp.append(eval('model.sum.ICM'+str(_)+'.B.B'))\n",
        "                                Out_model[x][y]=list_tmp\n",
        "                                # list_tmp contains [model.param_array, model.sum.ICM0.B.B,model.sum.ICM1.B.B])\n",
        "\n",
        "                                if check_param_names_flag:\n",
        "                                    model_parameter_names = model.parameter_names()\n",
        "                                    check_param_names_flag=False\n",
        "                            except:\n",
        "                                Out_QFlag[x,y]=False\n",
        "                                continue\n",
        "\n",
        "\n",
        "                            for out in range(noutputs):\n",
        "                                newX = Xtest.copy()\n",
        "\n",
        "                                newX = np.hstack([newX, out * np.ones((newX.shape[0], 1))])\n",
        "                                noise_dict = {'output_index': newX[:, -1:].astype(int)}\n",
        "                                Yp[:,None, out],Vp[:,None,out] =model.predict(newX, Y_metadata=noise_dict)\n",
        "\n",
        "                            if i_test==0:\n",
        "\n",
        "                                for ind in range(noutput_timeseries):\n",
        "                                    Out_mean[ind][:,None,x,y] = (Yp[:,None, 0]*Y_std_vec[ind]+Y_mean_vec[ind])/Nt\n",
        "                                    Out_unc[ind][:,None,x,y]  = (Vp[:,None, 0]*Y_std_vec[ind])/Nt\n",
        "\n",
        "                            else:\n",
        "                                for ind in range(noutput_timeseries):\n",
        "                                    Out_mean[ind][:,None,x,y] = Out_mean[ind][:,None,x,y] + (Yp[:,None, 0]*Y_std_vec[ind]+Y_mean_vec[ind])/Nt\n",
        "                                    Out_unc[ind][:,None,x,y]  = Out_unc[ind][:,None,x,y]  + (Vp[:,None, 0]*Y_std_vec[ind])/Nt\n",
        "\n",
        "                            del Yp,Vp\n",
        "    if check_param_names_flag:\n",
        "        print('\\n')\n",
        "        print(\"******************************************************************************************\")\n",
        "        print(\"No model has been trained within the processed block (probably due to non-valid input data)\".upper())\n",
        "        print(\"Processing is skippped to next block!\".upper())\n",
        "        print(\"******************************************************************************************\")\n",
        "        print('\\n')\n",
        "\n",
        "    return  Out_mean, Out_unc, Out_QFlag, Out_model,model_parameter_names"
      ],
      "metadata": {
        "id": "VYGrwf7Ya1pZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to run the MOGPR function.\n"
      ],
      "metadata": {
        "id": "asXt6S4idwt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main_loop(S3_LCC_flag = False, S3_FVC_flag = False, S3_LAI_flag= False,S3_FAPAR_flag= False,    # variable to MOGPR reconstruct\n",
        "              MODISLAI_flag = False, MODISFAPAR_flag= False, MODISNDVI_flag= False):                # Ancillary variable\n",
        "\n",
        "    feature = site\n",
        "\n",
        "    TIME = []   # Time list containing the dates for the reconstructing (S3) and predictor (MODIS) variables\n",
        "    DATA  = []  #  Variable values for reconstructing (S3) and predictor (MODIS) variables\n",
        "    TIME_str = []\n",
        "    legend_vec = []\n",
        "\n",
        "    print('S3TOAGPR time series being retrieved')\n",
        "    dfS3TOA,_ = extract_S3TOA_time_series(S3_LCC_flag,S3_LAI_flag,S3_FAPAR_flag,S3_FVC_flag,feature,start_date,end_date)\n",
        "    dfS3TOA = need_to_name(dfS3TOA)\n",
        "    TIME.append(datestr_to_number(dfS3TOA['date']))\n",
        "    TIME_str.append(dfS3TOA['date'])\n",
        "    DATA.append(np.reshape(dfS3TOA['LAI'].values,(dfS3TOA.shape[0],1,1)))\n",
        "    legend_vec.append('S3TOA')\n",
        "\n",
        "    if MODISLAI_flag:\n",
        "        print('MODISLAI time series being retrieved')\n",
        "        dfMODISLAI,_ = extract_MODISLAI_time_series(feature,start_date,end_date)\n",
        "        dfMODISLAI = need_to_name_modisdivLAI(dfMODISLAI)\n",
        "        TIME.append(datestr_to_number(dfMODISLAI['date']))\n",
        "        TIME_str.append(dfMODISLAI['date'])\n",
        "        DATA.append(np.reshape(dfMODISLAI['LAI'].values,(dfMODISLAI.shape[0],1,1)))\n",
        "        legend_vec.append('MODIS')\n",
        "        anc_variable = \"LAI\"\n",
        "\n",
        "    if MODISFAPAR_flag:\n",
        "        print('MODISFAPAR time series being retrieved')\n",
        "        dfMODISFAPAR,_ = extract_MODISFAPAR_time_series(feature,start_date,end_date)\n",
        "        dfMODISFAPAR = need_to_name_modisdivFAPAR(dfMODISFAPAR)\n",
        "        TIME.append(datestr_to_number(dfMODISFAPAR['date']))\n",
        "        TIME_str.append(dfMODISFAPAR['date'])\n",
        "        DATA.append(np.reshape(dfMODISFAPAR['FAPAR'].values,(dfMODISFAPAR.shape[0],1,1)))\n",
        "        legend_vec.append('MODIS')\n",
        "        anc_variable = \"FAPAR\"\n",
        "\n",
        "\n",
        "    if MODISNDVI_flag:\n",
        "        print('MODISNDVI time series being retrieved')\n",
        "        dfMODISNDVI,_ = extract_MODISNDVI_time_series(feature,start_date,end_date)\n",
        "        dfMODISNDVI = need_to_name_modisdivNDVI(dfMODISNDVI)\n",
        "        TIME.append(datestr_to_number(dfMODISNDVI['date']))\n",
        "        TIME_str.append(dfMODISNDVI['date'])\n",
        "        DATA.append(np.reshape(dfMODISNDVI['NDVI'].values,(dfMODISNDVI.shape[0],1,1)))\n",
        "        legend_vec.append('MODIS')\n",
        "        anc_variable = \"NDVI\"\n",
        "\n",
        "    Nout = len(DATA)\n",
        "    Nt =1\n",
        "\n",
        "    time_vec_MIN        = np.min(list(pd.core.common.flatten(TIME)))\n",
        "    time_vec_MAX        = np.max(list(pd.core.common.flatten(TIME)))\n",
        "    output_timevec      = np.array(range(int(time_vec_MIN),int(time_vec_MAX),Tstep),dtype=float)\n",
        "    output_time         = number_to_datestr(output_timevec)\n",
        "    outputs_timevec_str =[ _.strftime(\"%Y%m%d\") for _ in output_time]\n",
        "\n",
        "    print('MULTIOUTPUT Being calculated, please wait')\n",
        "    Master_Ind = 0\n",
        "    Out_mean, Out_unc, Out_QFlag, Out_model,model_parameter_names = GPY_retrieval_Noutput(DATA[:],TIME[:],Master_Ind,output_timevec,\n",
        "                                                                                                Nt,proc_line_print ='MOGPR modelling..')\n",
        "\n",
        "    plt.figure(figsize= (25,6))\n",
        "    sensor = legend_vec[0]\n",
        "\n",
        "    number_sensor_inputs = len(DATA)\n",
        "    for number in range(number_sensor_inputs):\n",
        "        p1 = plt.plot_date(TIME_str[number],DATA[number].ravel(),\"o\",label = legend_vec[number])\n",
        "\n",
        "\n",
        "    p2 = plt.plot_date(output_time, Out_mean[0].ravel(),\"-\",label = \"S3_reconstructed\")\n",
        "\n",
        "    plt.fill_between(output_time, Out_mean[0].ravel()+Out_unc[0].ravel(),\n",
        "                        Out_mean[0].ravel()-Out_unc[0].ravel(), alpha=0.5)\n",
        "    fontsize = 40\n",
        "\n",
        "    plt.yticks(fontsize=fontsize)\n",
        "    plt.xticks(fontsize=fontsize)\n",
        "    # plt.title(site_info[1]+\" \" + site_info[0]+\"  (\" +site_info[3]+\", \" + site_info[4]+\")  ancillary : \" +anc_variable ,fontsize=fontsize)\n",
        "    plt.ylabel(\"\",fontsize=fontsize)\n",
        "    plt.ylim([plotylim_min,plotylim_max])\n",
        "    plt.ylabel(exp_variable + \" \"+ exp_var_unit,fontsize=fontsize)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.legend(fontsize=20)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Out_mean = mean estimate output from the MOGPR algorithm\n",
        "    Out_unc = epistemic uncertainty associated with the MOGPR algorithm\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "Rf74EjJxdRS2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call the function, use the flag associated with the variable-sensor you want to use:\n",
        "\n",
        "For reconstruction: use S3 flags\n",
        "For predictor/guiding variable: use MODIS flags\n",
        "\n",
        "\n",
        "Here we are reconstructing S3 Based FVC with ancillary MODIS based FAPAR and NDVI time series."
      ],
      "metadata": {
        "id": "tJRo_GmYguVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_loop(S3_FVC_flag = True  # Reconstructing\n",
        "          ,MODISFAPAR_flag = True # Predictor 1\n",
        "          ,MODISNDVI_flag = False # Predictor 2\n",
        "          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb6KFRU_guAh",
        "outputId": "fef3d019-940d-48b7-8130-1d2d7a42061c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S3TOAGPR time series being retrieved\n",
            "MODISFAPAR time series being retrieved\n",
            "MULTIOUTPUT Being calculated, please wait\n"
          ]
        }
      ]
    }
  ]
}